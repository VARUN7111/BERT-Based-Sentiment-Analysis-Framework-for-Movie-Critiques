{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import tokenizers\n",
    "#!pip install utils\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Configuration\n",
    "class config:\n",
    "    MAX_LEN = 512\n",
    "    Train_Batch_Size = 8\n",
    "    Valid_Batch_Size = 4\n",
    "    Weight_Decay = 0.001\n",
    "    Learning_Rate = 3e-5\n",
    "    Epochs = 4\n",
    "    BERT_PATH = \"bert_base_uncased\"\n",
    "    MODEL_PATH = \"bert_model_f1.bin\"\n",
    "    TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloader for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset:\n",
    "    def __init__(self, review, target):\n",
    "        self.review = review\n",
    "        self.target = target\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.review[item])\n",
    "        review = \" \".join(review.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(review, None, add_special_tokens=True, max_length=self.max_len,pad_to_max_length=True) # this can encode two strings at a time. that's why its none.\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"] # this is always 1, mask & tokentypeids are the same in this case as we have only one string in input\n",
    "#         print(\"ids:\", ids)\n",
    "#         print(\"mask:\", mask)\n",
    "#         print(\"token_type_ids:\", token_type_ids)\n",
    "#         print(\"\\n\")\n",
    "#         print(\"torchhhh\" : torch.tensor(ids, dtype=torch.long))\n",
    "\n",
    "        return {\"ids\": torch.tensor(ids, dtype=torch.long), \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long), \"targets\": torch.tensor(self.target[item], dtype=torch.float)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentPredictor, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, return_dict=False)\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _,o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        bo = self.bert_drop(o2)\n",
    "        output = self.out(bo)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output of Bert model:**\n",
    "\n",
    "**Pooled Output:** Represents a fixed-size representation of the entire input sequence, usually used for classification tasks. This representation is obtained by applying a pooling operation on the output embeddings of the special [CLS] token.(pooled output from bert pooler layer)\n",
    "\n",
    "**Encoder Outputs:** Provide the final hidden states of all layers in the encoder stack. These outputs can be utilized for various purposes such as understanding attention mechanisms or extracting features for downstream tasks.(last hidden states- encoded outputs). sequence of hidden states for each token for all batches. (**\"tokens\"= 512, a batch = 512*768**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets = d[\"targets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            final_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            final_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist()) # sigmoid as we have the linearlayer\n",
    "\n",
    "    final_outputs = np.array(final_outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(final_targets, final_outputs)\n",
    "    f1 = metrics.f1_score(final_targets, final_outputs, average='micro')\n",
    "    return final_outputs, final_targets, accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "df_train, df_valid = model_selection.train_test_split(df, test_size=0.1, random_state=42, stratify=df.sentiment.values)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_valid = df_valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentPredictor(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert_drop): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = SentimentPredictor()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_model:\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    from torch.optim import AdamW\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "    # Define a grid of hyperparameters\n",
    "    param_grid = {\n",
    "        'learning_rate': [1e-5, 3e-5, 5e-5],\n",
    "        'train_batch_size': [4, 8, 16],\n",
    "        'valid_batch_size': [4, 8, 16],\n",
    "        'num_train_epochs': [3, 4, 6, 10],\n",
    "        'weight_decay': [0.0, 0.1, 0.001]\n",
    "    }\n",
    "\n",
    "    # Create a ParameterGrid instance\n",
    "    grid = ParameterGrid(param_grid)\n",
    "\n",
    "    # Variable to store the best accuracy and corresponding parameters\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for params in grid:\n",
    "        print(f\"Training with parameters: {params}\")\n",
    "\n",
    "        # Update the training loader and vaidation loader and model according to the current set of parameters\n",
    "        train_dataset = IMDBDataset(review=df_train.review.values, target=df_train.sentiment.values)\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params['train_batch_size'])\n",
    "        valid_dataset = IMDBDataset(review=df_valid.review.values, target=df_valid.sentiment.values)\n",
    "        valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=params['valid_batch_size'])\n",
    "\n",
    "        # Update the optimizer with current learning rate and weight decay\n",
    "        param_optimizer = list(model.named_parameters()) #default from transformers\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]#default from transformers\n",
    "        optimizer_parameters = [\n",
    "            {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": params['weight_decay']},\n",
    "            {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_parameters, lr=params['learning_rate'])\n",
    "\n",
    "        # Adjust the scheduler\n",
    "        num_train_steps = int(len(df_train) / params['train_batch_size'] * params['num_train_epochs'])\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "\n",
    "        # Initialize training loop\n",
    "        for epoch in range(params['num_train_epochs']):\n",
    "            train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "            outputs, targets = eval_fn(valid_data_loader, model, device)\n",
    "            outputs = np.array(outputs) >= 0.5\n",
    "            accuracy = metrics.accuracy_score(targets, outputs)\n",
    "            print(f\"Epoch = {epoch}, Accuracy Score = {accuracy}\")\n",
    "\n",
    "            # Check if the current model's accuracy is the best\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = params\n",
    "\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(review=df_train.review.values, target=df_train.sentiment.values)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.Train_Batch_Size)\n",
    "\n",
    "valid_dataset = IMDBDataset(review=df_valid.review.values, target=df_valid.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=config.Valid_Batch_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters()) #default from transformers\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]#default from transformers\n",
    "optimizer_parameters = [{ \"params\": [ p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\"weight_decay\": config.Weight_Decay},\n",
    "                        { \"params\": [ p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/34/shanmuv1/unix/.local/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = int(len(df_train) / config.Train_Batch_Size * config.Epochs)\n",
    "optimizer = AdamW(optimizer_parameters, lr=config.Learning_Rate) # lr=0.00003\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps) # scheduler from huggingface -simple one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5625 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/u/34/shanmuv1/unix/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5625/5625 [20:58<00:00,  4.47it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [00:52<00:00, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy Score = 0.9322, F1 Score = 0.9322\n",
      "Saved new best model with F1 Score: 0.9322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5625 [00:00<?, ?it/s]/u/34/shanmuv1/unix/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5625/5625 [20:59<00:00,  4.47it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [00:52<00:00, 23.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy Score = 0.9428, F1 Score = 0.9428\n",
      "Saved new best model with F1 Score: 0.9428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5625 [00:00<?, ?it/s]/u/34/shanmuv1/unix/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5625/5625 [20:59<00:00,  4.47it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [00:52<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy Score = 0.9438, F1 Score = 0.9438\n",
      "Saved new best model with F1 Score: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5625 [00:00<?, ?it/s]/u/34/shanmuv1/unix/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5625/5625 [21:00<00:00,  4.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [00:52<00:00, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy Score = 0.945, F1 Score = 0.945\n",
      "Saved new best model with F1 Score: 0.945\n"
     ]
    }
   ],
   "source": [
    "best_f1_score = 0\n",
    "for epoch in range(config.Epochs):\n",
    "    train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "    outputs, targets, accuracy, f1 = eval_fn(valid_data_loader, model, device)\n",
    "    print(f\"Epoch = {epoch}, Accuracy Score = {accuracy}, F1 Score = {f1}\")\n",
    "    if f1 > best_f1_score:\n",
    "        torch.save(model.state_dict(), config.MODEL_PATH)\n",
    "        best_f1_score = f1  # Update the best F1-score\n",
    "        print(f\"Saved new best model with F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [1, 2, 3, 4]\n",
    "f1_scores = [0.9322, 0.9428, 0.9438, 0.9450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy scores across epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, f1_scores, marker='o', linestyle='-', color='orange')  # Changed color to orange\n",
    "plt.title('F1 Scores Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
